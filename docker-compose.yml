version: "3.8"

services:
  # -------------------------------------------
  # LLM Service (local model backend)
  # -------------------------------------------
  llm:
    build: ./llm
    container_name: demo_llm
    network_mode: "host"
    environment:
      - OLLAMA_BASE=http://localhost:11434/api
      - OLLAMA_MODEL=llama3.2:1b
    # Optional: add restart policy for stability
    restart: unless-stopped

  # -------------------------------------------
  # Banking Agent Service (your main Flask app)
  # -------------------------------------------
  agent:
    build: ./agent
    container_name: demo_agent
    network_mode: "host"   # Using host mode ensures localhost:5002 works for LLM
    environment:
      - LLM_URL=http://localhost:5002/generate
      - AUTO_EXECUTE=true
      - RED_TEAM_MODE=false
      - DATA_PATH=/app/data/customers_transactions.json
    depends_on:
      - llm
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped

  # -------------------------------------------
  # Prompt Tester (optional for red teaming)
  # -------------------------------------------
  prompt:
    build: ./prompt_tests
    container_name: demo_prompt
    command: ["python", "prompt_tests.py"]
    depends_on:
      - llm
      - agent
    restart: "no"
